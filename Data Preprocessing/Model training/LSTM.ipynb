{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, max_error\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import yeojohnson, boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1221 entries, 0 to 1220\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Business ID                               1221 non-null   object \n",
      " 1   Name                                      1221 non-null   object \n",
      " 2   Latitude                                  1221 non-null   float64\n",
      " 3   Longitude                                 1221 non-null   float64\n",
      " 4   Category                                  1221 non-null   object \n",
      " 5   Rating                                    1221 non-null   object \n",
      " 6   Popularity                                1221 non-null   float64\n",
      " 7   Google Place ID                           1221 non-null   object \n",
      " 8   Business Status                           1221 non-null   object \n",
      " 9   Distance (m)                              1221 non-null   float64\n",
      " 10  Cluster                                   77 non-null     float64\n",
      " 11  generalCategory                           1221 non-null   object \n",
      " 12  Religious Institutions                    1221 non-null   int64  \n",
      " 13  Coffee Shops                              1221 non-null   int64  \n",
      " 14  Food & Dining                             1221 non-null   int64  \n",
      " 15  Restaurants                               1221 non-null   int64  \n",
      " 16  Home & Construction Services              1221 non-null   int64  \n",
      " 17  Entertainment & Recreation                1221 non-null   int64  \n",
      " 18  Retail & Shopping                         1221 non-null   int64  \n",
      " 19  Finance & Services                        1221 non-null   int64  \n",
      " 20  Education                                 1221 non-null   int64  \n",
      " 21  Health                                    1221 non-null   int64  \n",
      " 22  Public & Government Services              1221 non-null   int64  \n",
      " 23  Hotels & Hospitality                      1221 non-null   int64  \n",
      " 24  Transportation & Travel                   1221 non-null   int64  \n",
      " 25  Beauty & Wellness                         1221 non-null   int64  \n",
      " 26  POI Density                               1221 non-null   int64  \n",
      " 27  Avg Rating - Business Type                1190 non-null   float64\n",
      " 28  Avg Rating - Food & Dining                1212 non-null   float64\n",
      " 29  Competition - Business Type/Area          1221 non-null   float64\n",
      " 30  Competition - Food & Dining/Area          1221 non-null   float64\n",
      " 31  Competition - Business Type/POI Density   1221 non-null   float64\n",
      " 32  Competition - Food & Dining/POI Density   1221 non-null   float64\n",
      " 33  Competition - Business Type/related POIs  1221 non-null   float64\n",
      " 34  Competition - Food & Dining/related POIs  1221 non-null   float64\n",
      " 35  Population Within 1km                     1221 non-null   int64  \n",
      "dtypes: float64(13), int64(16), object(7)\n",
      "memory usage: 343.5+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 689 entries, 0 to 688\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Business ID                               689 non-null    object \n",
      " 1   Name                                      689 non-null    object \n",
      " 2   Latitude                                  689 non-null    float64\n",
      " 3   Longitude                                 689 non-null    float64\n",
      " 4   Category                                  689 non-null    object \n",
      " 5   Rating                                    689 non-null    object \n",
      " 6   Popularity                                689 non-null    float64\n",
      " 7   Google Place ID                           688 non-null    object \n",
      " 8   Business Status                           689 non-null    object \n",
      " 9   Distance (m)                              689 non-null    float64\n",
      " 10  Cluster                                   51 non-null     float64\n",
      " 11  generalCategory                           689 non-null    object \n",
      " 12  Religious Institutions                    689 non-null    int64  \n",
      " 13  Coffee Shops                              689 non-null    int64  \n",
      " 14  Food & Dining                             689 non-null    int64  \n",
      " 15  Restaurants                               689 non-null    int64  \n",
      " 16  Home & Construction Services              689 non-null    int64  \n",
      " 17  Entertainment & Recreation                689 non-null    int64  \n",
      " 18  Retail & Shopping                         689 non-null    int64  \n",
      " 19  Finance & Services                        689 non-null    int64  \n",
      " 20  Education                                 689 non-null    int64  \n",
      " 21  Health                                    689 non-null    int64  \n",
      " 22  Public & Government Services              689 non-null    int64  \n",
      " 23  Hotels & Hospitality                      689 non-null    int64  \n",
      " 24  Transportation & Travel                   689 non-null    int64  \n",
      " 25  Beauty & Wellness                         689 non-null    int64  \n",
      " 26  POI Density                               689 non-null    int64  \n",
      " 27  Avg Rating - Business Type                616 non-null    float64\n",
      " 28  Avg Rating - Food & Dining                683 non-null    float64\n",
      " 29  Competition - Business Type/Area          689 non-null    float64\n",
      " 30  Competition - Food & Dining/Area          689 non-null    float64\n",
      " 31  Competition - Business Type/POI Density   689 non-null    float64\n",
      " 32  Competition - Food & Dining/POI Density   689 non-null    float64\n",
      " 33  Competition - Business Type/related POIs  689 non-null    float64\n",
      " 34  Competition - Food & Dining/related POIs  689 non-null    float64\n",
      " 35  Population Within 1km                     689 non-null    int64  \n",
      "dtypes: float64(13), int64(16), object(7)\n",
      "memory usage: 193.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "restaurants_df = pd.read_csv(\"../Foursquare/final_restaurants_dataset_cleaned.csv\")\n",
    "coffeeshops_df = pd.read_csv(\"../Foursquare/final_coffeeshops_dataset_cleaned.csv\")\n",
    "print(restaurants_df.info())\n",
    "print(coffeeshops_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([coffeeshops_df, restaurants_df], ignore_index=True)\n",
    "combined_df[\"generalCategory\"] = combined_df[\"generalCategory\"].map({\"coffee shop\": 0, \"restaurant\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1910 entries, 0 to 1909\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Business ID                               1910 non-null   object \n",
      " 1   Name                                      1910 non-null   object \n",
      " 2   Latitude                                  1910 non-null   float64\n",
      " 3   Longitude                                 1910 non-null   float64\n",
      " 4   Category                                  1910 non-null   object \n",
      " 5   Rating                                    1910 non-null   object \n",
      " 6   Popularity                                1910 non-null   float64\n",
      " 7   Google Place ID                           1909 non-null   object \n",
      " 8   Business Status                           1910 non-null   object \n",
      " 9   Distance (m)                              1910 non-null   float64\n",
      " 10  Cluster                                   128 non-null    float64\n",
      " 11  generalCategory                           1910 non-null   int64  \n",
      " 12  Religious Institutions                    1910 non-null   int64  \n",
      " 13  Coffee Shops                              1910 non-null   int64  \n",
      " 14  Food & Dining                             1910 non-null   int64  \n",
      " 15  Restaurants                               1910 non-null   int64  \n",
      " 16  Home & Construction Services              1910 non-null   int64  \n",
      " 17  Entertainment & Recreation                1910 non-null   int64  \n",
      " 18  Retail & Shopping                         1910 non-null   int64  \n",
      " 19  Finance & Services                        1910 non-null   int64  \n",
      " 20  Education                                 1910 non-null   int64  \n",
      " 21  Health                                    1910 non-null   int64  \n",
      " 22  Public & Government Services              1910 non-null   int64  \n",
      " 23  Hotels & Hospitality                      1910 non-null   int64  \n",
      " 24  Transportation & Travel                   1910 non-null   int64  \n",
      " 25  Beauty & Wellness                         1910 non-null   int64  \n",
      " 26  POI Density                               1910 non-null   int64  \n",
      " 27  Avg Rating - Business Type                1806 non-null   float64\n",
      " 28  Avg Rating - Food & Dining                1895 non-null   float64\n",
      " 29  Competition - Business Type/Area          1910 non-null   float64\n",
      " 30  Competition - Food & Dining/Area          1910 non-null   float64\n",
      " 31  Competition - Business Type/POI Density   1910 non-null   float64\n",
      " 32  Competition - Food & Dining/POI Density   1910 non-null   float64\n",
      " 33  Competition - Business Type/related POIs  1910 non-null   float64\n",
      " 34  Competition - Food & Dining/related POIs  1910 non-null   float64\n",
      " 35  Population Within 1km                     1910 non-null   int64  \n",
      "dtypes: float64(13), int64(17), object(6)\n",
      "memory usage: 537.3+ KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "          Latitude    Longitude   Popularity  Distance (m)     Cluster  \\\n",
      "count  1910.000000  1910.000000  1910.000000   1910.000000  128.000000   \n",
      "mean     26.363874    50.123839     0.910267     37.996560    2.890625   \n",
      "std       0.077687     0.069525     0.185596    447.398385    2.820717   \n",
      "min      26.145103    49.962525     0.017061      0.000000   -1.000000   \n",
      "25%      26.305725    50.061163     0.934768     11.023039    0.000000   \n",
      "50%      26.369599    50.117536     0.973601     18.733289    3.000000   \n",
      "75%      26.421916    50.193904     0.990967     33.603023    5.000000   \n",
      "max      26.513218    50.229701     0.999995  19539.998362    8.000000   \n",
      "\n",
      "       generalCategory  Religious Institutions  Coffee Shops  Food & Dining  \\\n",
      "count      1910.000000             1910.000000   1910.000000    1910.000000   \n",
      "mean          0.639267                0.850262      2.752880      10.904188   \n",
      "std           0.480339                1.209179      2.687822       6.112421   \n",
      "min           0.000000                0.000000      0.000000       0.000000   \n",
      "25%           0.000000                0.000000      1.000000       6.000000   \n",
      "50%           1.000000                0.000000      2.000000      10.000000   \n",
      "75%           1.000000                1.000000      4.000000      15.000000   \n",
      "max           1.000000                9.000000     18.000000      29.000000   \n",
      "\n",
      "       Restaurants  ...  POI Density  Avg Rating - Business Type  \\\n",
      "count  1910.000000  ...  1910.000000                 1806.000000   \n",
      "mean      7.883246  ...    57.842408                    4.041915   \n",
      "std       4.727402  ...     1.995223                    0.270169   \n",
      "min       0.000000  ...    28.000000                    1.000000   \n",
      "25%       4.000000  ...    58.000000                    3.880000   \n",
      "50%       7.000000  ...    58.000000                    4.006905   \n",
      "75%      11.000000  ...    58.000000                    4.200000   \n",
      "max      22.000000  ...    60.000000                    5.000000   \n",
      "\n",
      "       Avg Rating - Food & Dining  Competition - Business Type/Area  \\\n",
      "count                 1895.000000                       1910.000000   \n",
      "mean                     4.028150                          2.122172   \n",
      "std                      0.223695                          1.563167   \n",
      "min                      1.000000                          0.000000   \n",
      "25%                      3.914550                          0.954927   \n",
      "50%                      4.025000                          1.909855   \n",
      "75%                      4.157143                          3.183091   \n",
      "max                      4.925000                          7.002801   \n",
      "\n",
      "       Competition - Food & Dining/Area  \\\n",
      "count                       1910.000000   \n",
      "mean                           3.470903   \n",
      "std                            1.945639   \n",
      "min                            0.000000   \n",
      "25%                            1.909855   \n",
      "50%                            3.183091   \n",
      "75%                            4.774637   \n",
      "max                            9.230965   \n",
      "\n",
      "       Competition - Business Type/POI Density  \\\n",
      "count                              1910.000000   \n",
      "mean                                  0.115313   \n",
      "std                                   0.084851   \n",
      "min                                   0.000000   \n",
      "25%                                   0.050847   \n",
      "50%                                   0.103448   \n",
      "75%                                   0.172414   \n",
      "max                                   0.372881   \n",
      "\n",
      "       Competition - Food & Dining/POI Density  \\\n",
      "count                              1910.000000   \n",
      "mean                                  0.188600   \n",
      "std                                   0.105558   \n",
      "min                                   0.000000   \n",
      "25%                                   0.103448   \n",
      "50%                                   0.175439   \n",
      "75%                                   0.258621   \n",
      "max                                   0.500000   \n",
      "\n",
      "       Competition - Business Type/related POIs  \\\n",
      "count                               1910.000000   \n",
      "mean                                   0.215970   \n",
      "std                                    0.141153   \n",
      "min                                    0.000000   \n",
      "25%                                    0.100000   \n",
      "50%                                    0.200000   \n",
      "75%                                    0.315789   \n",
      "max                                    0.687500   \n",
      "\n",
      "       Competition - Food & Dining/related POIs  Population Within 1km  \n",
      "count                               1910.000000            1910.000000  \n",
      "mean                                   0.351720           27070.957592  \n",
      "std                                    0.153512           22675.111631  \n",
      "min                                    0.000000               0.000000  \n",
      "25%                                    0.240000           13047.000000  \n",
      "50%                                    0.361111           20202.500000  \n",
      "75%                                    0.464286           35456.000000  \n",
      "max                                    0.750000          110993.000000  \n",
      "\n",
      "[8 rows x 30 columns]\n",
      "\n",
      "Missing Values:\n",
      "Business ID                                    0\n",
      "Name                                           0\n",
      "Latitude                                       0\n",
      "Longitude                                      0\n",
      "Category                                       0\n",
      "Rating                                         0\n",
      "Popularity                                     0\n",
      "Google Place ID                                1\n",
      "Business Status                                0\n",
      "Distance (m)                                   0\n",
      "Cluster                                     1782\n",
      "generalCategory                                0\n",
      "Religious Institutions                         0\n",
      "Coffee Shops                                   0\n",
      "Food & Dining                                  0\n",
      "Restaurants                                    0\n",
      "Home & Construction Services                   0\n",
      "Entertainment & Recreation                     0\n",
      "Retail & Shopping                              0\n",
      "Finance & Services                             0\n",
      "Education                                      0\n",
      "Health                                         0\n",
      "Public & Government Services                   0\n",
      "Hotels & Hospitality                           0\n",
      "Transportation & Travel                        0\n",
      "Beauty & Wellness                              0\n",
      "POI Density                                    0\n",
      "Avg Rating - Business Type                   104\n",
      "Avg Rating - Food & Dining                    15\n",
      "Competition - Business Type/Area               0\n",
      "Competition - Food & Dining/Area               0\n",
      "Competition - Business Type/POI Density        0\n",
      "Competition - Food & Dining/POI Density        0\n",
      "Competition - Business Type/related POIs       0\n",
      "Competition - Food & Dining/related POIs       0\n",
      "Population Within 1km                          0\n",
      "dtype: int64\n",
      "\n",
      "Final Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 359 entries, 3 to 1889\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Business ID                               359 non-null    object \n",
      " 1   Name                                      359 non-null    object \n",
      " 2   Latitude                                  359 non-null    float64\n",
      " 3   Longitude                                 359 non-null    float64\n",
      " 4   Category                                  359 non-null    object \n",
      " 5   Rating                                    359 non-null    object \n",
      " 6   Popularity                                359 non-null    float64\n",
      " 7   Google Place ID                           359 non-null    object \n",
      " 8   Business Status                           359 non-null    object \n",
      " 9   Distance (m)                              359 non-null    float64\n",
      " 10  Cluster                                   359 non-null    float64\n",
      " 11  generalCategory                           359 non-null    int64  \n",
      " 12  Religious Institutions                    359 non-null    int64  \n",
      " 13  Coffee Shops                              359 non-null    int64  \n",
      " 14  Food & Dining                             359 non-null    int64  \n",
      " 15  Restaurants                               359 non-null    int64  \n",
      " 16  Home & Construction Services              359 non-null    int64  \n",
      " 17  Entertainment & Recreation                359 non-null    int64  \n",
      " 18  Retail & Shopping                         359 non-null    int64  \n",
      " 19  Finance & Services                        359 non-null    int64  \n",
      " 20  Education                                 359 non-null    int64  \n",
      " 21  Health                                    359 non-null    int64  \n",
      " 22  Public & Government Services              359 non-null    int64  \n",
      " 23  Hotels & Hospitality                      359 non-null    int64  \n",
      " 24  Transportation & Travel                   359 non-null    int64  \n",
      " 25  Beauty & Wellness                         359 non-null    int64  \n",
      " 26  POI Density                               359 non-null    int64  \n",
      " 27  Avg Rating - Business Type                359 non-null    float64\n",
      " 28  Avg Rating - Food & Dining                359 non-null    float64\n",
      " 29  Competition - Business Type/Area          359 non-null    float64\n",
      " 30  Competition - Food & Dining/Area          359 non-null    float64\n",
      " 31  Competition - Business Type/POI Density   359 non-null    float64\n",
      " 32  Competition - Food & Dining/POI Density   359 non-null    float64\n",
      " 33  Competition - Business Type/related POIs  359 non-null    float64\n",
      " 34  Competition - Food & Dining/related POIs  359 non-null    float64\n",
      " 35  Population Within 1km                     359 non-null    int64  \n",
      "dtypes: float64(13), int64(17), object(6)\n",
      "memory usage: 103.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Initial Dataset Info:\")\n",
    "print(combined_df.info())\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(combined_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with median values\n",
    "num_cols = combined_df.select_dtypes(include=['number']).columns\n",
    "combined_df[num_cols] = combined_df[num_cols].fillna(combined_df[num_cols].median())\n",
    "\n",
    "# Fill categorical columns with mode (most frequent value)\n",
    "cat_cols = combined_df.select_dtypes(include=['object']).columns\n",
    "combined_df[cat_cols] = combined_df[cat_cols].fillna(combined_df[cat_cols].mode().iloc[0])\n",
    "\n",
    "# Remove duplicate rows if any\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "Q1 = combined_df[num_cols].quantile(0.25)\n",
    "Q3 = combined_df[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "combined_df = combined_df[~((combined_df[num_cols] < (Q1 - 1.5 * IQR)) | (combined_df[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Display final info after fixing issues\n",
    "print(\"\\nFinal Dataset Info:\")\n",
    "print(combined_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(combined_df[\"Avg Rating - Business Type\"], kde=True)\n",
    "plt.show()\n",
    "sns.histplot(combined_df[\"Avg Rating - Food & Dining\"], kde=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_columns = ['Business ID', 'Name', 'Latitude', 'Longitude', 'Category', 'Rating', \n",
    "                      'Google Place ID', 'Business Status', 'Distance (m)', 'Cluster']\n",
    "combined_df.drop(columns=irrelevant_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_cols = [\n",
    "    \"Competition - Business Type/Area\",\n",
    "    \"Competition - Food & Dining/Area\",\n",
    "    \"Competition - Business Type/POI Density\",\n",
    "    \"Competition - Food & Dining/POI Density\",\n",
    "    \"Competition - Business Type/related POIs\",\n",
    "    \"Competition - Food & Dining/related POIs\"\n",
    "]\n",
    "\n",
    "avg_rating_cols = [\"Avg Rating - Business Type\", \"Avg Rating - Food & Dining\"]\n",
    "\n",
    "target_col = \"Popularity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity                                  0\n",
      "generalCategory                             0\n",
      "Religious Institutions                      0\n",
      "Coffee Shops                                0\n",
      "Food & Dining                               0\n",
      "Restaurants                                 0\n",
      "Home & Construction Services                0\n",
      "Entertainment & Recreation                  0\n",
      "Retail & Shopping                           0\n",
      "Finance & Services                          0\n",
      "Education                                   0\n",
      "Health                                      0\n",
      "Public & Government Services                0\n",
      "Hotels & Hospitality                        0\n",
      "Transportation & Travel                     0\n",
      "Beauty & Wellness                           0\n",
      "POI Density                                 0\n",
      "Avg Rating - Business Type                  0\n",
      "Avg Rating - Food & Dining                  0\n",
      "Competition - Business Type/Area            0\n",
      "Competition - Food & Dining/Area            0\n",
      "Competition - Business Type/POI Density     0\n",
      "Competition - Food & Dining/POI Density     0\n",
      "Competition - Business Type/related POIs    0\n",
      "Competition - Food & Dining/related POIs    0\n",
      "Population Within 1km                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.isnull().sum())  # Check for NaNs\n",
    "#df = df.dropna()  # Drop any missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions to remove outliers and highly correlated features\n",
    "\n",
    "def remove_outliers(X, y):\n",
    "    \"\"\"Remove outliers using IQR.\"\"\"\n",
    "    Q1 = np.percentile(y, 25)\n",
    "    Q3 = np.percentile(y, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    mask = (y >= lower_bound) & (y <= upper_bound)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "    \n",
    "# Remove Highly Correlated Features (Modified)\n",
    "removed_corr_features = []  # List to store removed features\n",
    "\n",
    "def remove_highly_correlated_features(X, threshold=0.95):\n",
    "    \"\"\"Removes highly correlated features from X.\"\"\"\n",
    "    corr_matrix = X.corr().abs()  # Compute absolute correlation matrix\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))  # Upper triangle\n",
    "\n",
    "    # Find columns to drop\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    return X.drop(columns=to_drop, errors='ignore'), to_drop  # Drop and return removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.20.3\n",
      "Uninstalling protobuf-3.20.3:\n",
      "  Successfully uninstalled protobuf-3.20.3\n",
      "Collecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/162.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/162.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/162.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/162.1 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/162.1 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 30.7/162.1 kB 146.3 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 41.0/162.1 kB 151.3 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 61.4/162.1 kB 204.8 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 71.7/162.1 kB 218.6 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 122.9/162.1 kB 327.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 162.1/162.1 kB 405.0 kB/s eta 0:00:00\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-3.20.3\n",
      "Requirement already satisfied: tensorflow in c:\\users\\jory\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jory\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\jory\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jory\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jory\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf -y\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (200, 17), X_test shape: (51, 17)\n",
      "y_train shape: (200,), y_test shape: (51,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 50ms/step - loss: 0.3623 - mae: 0.5349 - val_loss: 0.2615 - val_mae: 0.4349\n",
      "Epoch 2/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2857 - mae: 0.4715 - val_loss: 0.1397 - val_mae: 0.2990\n",
      "Epoch 3/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1390 - mae: 0.3138 - val_loss: 0.0801 - val_mae: 0.2483\n",
      "Epoch 4/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1109 - mae: 0.2820 - val_loss: 0.0803 - val_mae: 0.2492\n",
      "Epoch 5/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1070 - mae: 0.2756 - val_loss: 0.0770 - val_mae: 0.2420\n",
      "Epoch 6/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0837 - mae: 0.2487 - val_loss: 0.0761 - val_mae: 0.2410\n",
      "Epoch 7/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1046 - mae: 0.2762 - val_loss: 0.0755 - val_mae: 0.2399\n",
      "Epoch 8/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0862 - mae: 0.2514 - val_loss: 0.0751 - val_mae: 0.2393\n",
      "Epoch 9/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0918 - mae: 0.2585 - val_loss: 0.0753 - val_mae: 0.2391\n",
      "Epoch 10/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0878 - mae: 0.2460 - val_loss: 0.0755 - val_mae: 0.2393\n",
      "Epoch 11/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0798 - mae: 0.2358 - val_loss: 0.0748 - val_mae: 0.2380\n",
      "Epoch 12/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0888 - mae: 0.2515 - val_loss: 0.0746 - val_mae: 0.2374\n",
      "Epoch 13/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0930 - mae: 0.2578 - val_loss: 0.0752 - val_mae: 0.2374\n",
      "Epoch 14/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0872 - mae: 0.2472 - val_loss: 0.0747 - val_mae: 0.2370\n",
      "Epoch 15/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0896 - mae: 0.2535 - val_loss: 0.0747 - val_mae: 0.2367\n",
      "Epoch 16/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0852 - mae: 0.2498 - val_loss: 0.0748 - val_mae: 0.2366\n",
      "Epoch 17/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0971 - mae: 0.2642 - val_loss: 0.0748 - val_mae: 0.2364\n",
      "Epoch 18/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0783 - mae: 0.2376 - val_loss: 0.0758 - val_mae: 0.2369\n",
      "Epoch 19/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0818 - mae: 0.2435 - val_loss: 0.0753 - val_mae: 0.2361\n",
      "Epoch 20/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0886 - mae: 0.2506 - val_loss: 0.0753 - val_mae: 0.2360\n",
      "Epoch 21/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0738 - mae: 0.2312 - val_loss: 0.0755 - val_mae: 0.2357\n",
      "Epoch 22/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0799 - mae: 0.2415 - val_loss: 0.0758 - val_mae: 0.2356\n",
      "Epoch 23/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0854 - mae: 0.2453 - val_loss: 0.0759 - val_mae: 0.2355\n",
      "Epoch 24/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0774 - mae: 0.2339 - val_loss: 0.0761 - val_mae: 0.2354\n",
      "Epoch 25/25\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0853 - mae: 0.2469 - val_loss: 0.0767 - val_mae: 0.2352\n",
      "LSTM model trained successfully!\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371ms/step\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.2769\n",
      "MSE             │     0.0767\n",
      "MAE             │     0.2352\n",
      "R²              │    -0.0077\n",
      "MedAE           │     0.2187\n",
      "Max Error       │     0.5116\n",
      "MAD             │     0.2418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, probplot\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to remove highly correlated features\n",
    "def remove_highly_correlated_features(X, threshold=0.95):\n",
    "    \"\"\"Remove features that are highly correlated with each other.\"\"\"\n",
    "    X_df = pd.DataFrame(X)\n",
    "    corr_matrix = X_df.corr().abs()\n",
    "    \n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    return X_df.drop(columns=to_drop, errors='ignore')\n",
    "\n",
    "# Assuming combined_df is your DataFrame\n",
    "transformed_df = restaurants_df.copy()\n",
    "\n",
    "# Remove outliers in 'Popularity'\n",
    "Q1 = transformed_df['Popularity'].quantile(0.25)\n",
    "Q3 = transformed_df['Popularity'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "transformed_df = transformed_df[(transformed_df['Popularity'] >= lower_bound) & (transformed_df['Popularity'] <= upper_bound)]\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = transformed_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Apply PowerTransformer (normalize distribution)\n",
    "power_transformer = PowerTransformer()\n",
    "transformed_df[num_cols] = power_transformer.fit_transform(transformed_df[num_cols])\n",
    "\n",
    "# Apply MinMaxScaler (scale to 0-1)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "transformed_df[num_cols] = minmax_scaler.fit_transform(transformed_df[num_cols])\n",
    "\n",
    "# Feature Selection\n",
    "feature_cols = [\n",
    "    \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "    \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "    \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "    \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "    \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "    \"Population Within 1km\"\n",
    "]\n",
    "\n",
    "# Ensure selected features exist in dataset\n",
    "X = transformed_df[feature_cols].copy()\n",
    "y = transformed_df['Popularity']\n",
    "\n",
    "# Apply the correlation-based feature removal function\n",
    "X = remove_highly_correlated_features(X, threshold=0.90)\n",
    "\n",
    "# Split data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Reshape input data for LSTM (samples, timesteps, features)\n",
    "X_train = np.expand_dims(X_train, axis=1)  # Reshape to (samples, timesteps=1, features)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Build LSTM model\n",
    "# Build LSTM model with updated parameters\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(16, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "# Compile model with updated learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0010), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model with updated epochs\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "print(\"LSTM model trained successfully!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, max_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "max_err = max_error(y_test, y_pred)\n",
    "mad = np.mean(np.abs(y_test - np.mean(y_test)))  # Mean Absolute Deviation\n",
    "\n",
    "# Print Results in Table Format\n",
    "print(f\"{'Metric':<15} │ {'Value':>10}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=25):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to train LSTM with fixed parameters\n",
    "def train_lstm(df):\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # Define sequence length (fixed)\n",
    "    sequence_length = 25\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train.values, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test.values, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Build LSTM Model with fixed parameters\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=(sequence_length, X_train.shape[1])),\n",
    "        Dropout(0.2),\n",
    "        LSTM(16, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0010), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Training with early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(X_train_seq, y_train_seq, epochs=25, batch_size=16, validation_data=(X_test_seq, y_test_seq),\n",
    "                        verbose=1, callbacks=[early_stop])\n",
    "\n",
    "    print(\"LSTM model trained successfully!\")\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "\n",
    "    # Compute Metrics\n",
    "    mae = mean_absolute_error(y_test_seq, y_pred)\n",
    "    mse = mean_squared_error(y_test_seq, y_pred)\n",
    "    r2 = r2_score(y_test_seq, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    medae = median_absolute_error(y_test_seq, y_pred)\n",
    "    max_err = max_error(y_test_seq, y_pred)\n",
    "    mad = np.mean(np.abs(y_test_seq - np.mean(y_test_seq)))  # Mean Absolute Deviation\n",
    "\n",
    "    # Print Results in Table Format\n",
    "    print(f\"{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 131ms/step - loss: 0.8735 - mae: 0.9297 - val_loss: 0.2517 - val_mae: 0.5010\n",
      "Epoch 2/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2632 - mae: 0.4751 - val_loss: 0.0018 - val_mae: 0.0376\n",
      "Epoch 3/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0369 - mae: 0.1541 - val_loss: 0.0211 - val_mae: 0.1426\n",
      "Epoch 4/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0372 - mae: 0.1580 - val_loss: 7.3912e-04 - val_mae: 0.0234\n",
      "Epoch 5/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0342 - mae: 0.1467 - val_loss: 9.7350e-04 - val_mae: 0.0223\n",
      "Epoch 6/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0251 - mae: 0.1197 - val_loss: 8.7262e-04 - val_mae: 0.0218\n",
      "Epoch 7/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0358 - mae: 0.1547 - val_loss: 0.0014 - val_mae: 0.0314\n",
      "Epoch 8/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0233 - mae: 0.1214 - val_loss: 0.0027 - val_mae: 0.0447\n",
      "Epoch 9/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0287 - mae: 0.1403 - val_loss: 7.8912e-04 - val_mae: 0.0247\n",
      "LSTM model trained successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0272\n",
      "MSE             │     0.0007\n",
      "MAE             │     0.0234\n",
      "R²              │     0.0097\n",
      "MedAE           │     0.0224\n",
      "Max Error       │     0.0620\n",
      "MAD             │     0.0240\n"
     ]
    }
   ],
   "source": [
    "lstm_model = train_lstm(restaurants_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=25):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to train LSTM with fixed best parameters\n",
    "def train_lstm(df):\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # Fixed sequence length\n",
    "    sequence_length = 25\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train.values, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test.values, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Build LSTM Model with fixed best parameters\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(16, return_sequences=False)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0010), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    # Train the model with fixed parameters\n",
    "    history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=16, validation_data=(X_test_seq, y_test_seq),\n",
    "                        verbose=1, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "    print(\"LSTM model trained successfully!\")\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "\n",
    "    # Compute Metrics\n",
    "    mae = mean_absolute_error(y_test_seq, y_pred)\n",
    "    mse = mean_squared_error(y_test_seq, y_pred)\n",
    "    r2 = r2_score(y_test_seq, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    medae = median_absolute_error(y_test_seq, y_pred)\n",
    "    max_err = max_error(y_test_seq, y_pred)\n",
    "    mad = np.mean(np.abs(y_test_seq - np.mean(y_test_seq)))  # Mean Absolute Deviation\n",
    "\n",
    "    # Print Results in Table Format\n",
    "    print(f\"{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - loss: 0.3952 - mae: 0.5477 - val_loss: 0.0249 - val_mae: 0.1559 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0592 - mae: 0.2053 - val_loss: 0.0072 - val_mae: 0.0812 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0296 - mae: 0.1406 - val_loss: 0.0065 - val_mae: 0.0765 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0280 - mae: 0.1302 - val_loss: 6.3855e-04 - val_mae: 0.0236 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0225 - mae: 0.1217 - val_loss: 7.3643e-04 - val_mae: 0.0244 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0219 - mae: 0.1165 - val_loss: 0.0083 - val_mae: 0.0876 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0172 - mae: 0.1091 - val_loss: 0.0024 - val_mae: 0.0424 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0123 - mae: 0.0885 - val_loss: 0.0049 - val_mae: 0.0656 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m5/8\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0119 - mae: 0.0825\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0119 - mae: 0.0820 - val_loss: 0.0017 - val_mae: 0.0351 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0125 - mae: 0.0930 - val_loss: 0.0052 - val_mae: 0.0680 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0103 - mae: 0.0818 - val_loss: 0.0060 - val_mae: 0.0736 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0154 - mae: 0.0960 - val_loss: 0.0025 - val_mae: 0.0442 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0125 - mae: 0.0940 - val_loss: 0.0084 - val_mae: 0.0886 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0168 - mae: 0.0970\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0158 - mae: 0.0940 - val_loss: 0.0078 - val_mae: 0.0850 - learning_rate: 5.0000e-04\n",
      "LSTM model trained successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768ms/step\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0253\n",
      "MSE             │     0.0006\n",
      "MAE             │     0.0236\n",
      "R²              │     0.1445\n",
      "MedAE           │     0.0225\n",
      "Max Error       │     0.0412\n",
      "MAD             │     0.0240\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm(restaurants_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error, median_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=20):  # Reduced sequence length\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to train optimized LSTM model\n",
    "def train_lstm(df):\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Normalize features using StandardScaler\n",
    "    feature_scaler = MaxAbsScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "    # Normalize the target variable\n",
    "    target_scaler = MaxAbsScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # Adjusted sequence length\n",
    "    sequence_length = 25  # Reduced from 25 to minimize noise\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Build LSTM Model with fixed best parameters\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(16, return_sequences=False)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0010), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    # Train the model with fixed parameters\n",
    "    history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=16, validation_data=(X_test_seq, y_test_seq),\n",
    "                        verbose=1, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "    print(\"LSTM model trained successfully!\")\n",
    "\n",
    "    # Predictions (inverse transform back to original scale)\n",
    "    y_pred_scaled = model.predict(X_test_seq)\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled)  # Convert predictions back to original scale\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))  # Convert actual values back\n",
    "\n",
    "    # Compute Metrics\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "    mse = mean_squared_error(y_test_actual, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_actual, y_pred)\n",
    "    medae = median_absolute_error(y_test_actual, y_pred)\n",
    "    max_err = max_error(y_test_actual, y_pred)\n",
    "    mad = np.mean(np.abs(y_test_actual - np.mean(y_test_actual)))  # Mean Absolute Deviation\n",
    "\n",
    "    # Print Results in Table Format\n",
    "    print(f\"\\n{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 274ms/step - loss: 0.7459 - mae: 0.8264 - val_loss: 0.0075 - val_mae: 0.0819 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0698 - mae: 0.2219 - val_loss: 0.0019 - val_mae: 0.0378 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0444 - mae: 0.1709 - val_loss: 0.0010 - val_mae: 0.0278 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0228 - mae: 0.1210 - val_loss: 0.0011 - val_mae: 0.0236 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0329 - mae: 0.1415 - val_loss: 0.0015 - val_mae: 0.0323 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0233 - mae: 0.1165 - val_loss: 0.0013 - val_mae: 0.0302 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0310 - mae: 0.1412 - val_loss: 8.3635e-04 - val_mae: 0.0259 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0224 - mae: 0.1215 - val_loss: 0.0079 - val_mae: 0.0842 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0166 - mae: 0.0987 - val_loss: 0.0020 - val_mae: 0.0391 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0125 - mae: 0.0900 - val_loss: 0.0077 - val_mae: 0.0835 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0146 - mae: 0.1017 - val_loss: 0.0085 - val_mae: 0.0878 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0130 - mae: 0.0854\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0132 - mae: 0.0866 - val_loss: 0.0064 - val_mae: 0.0749 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0146 - mae: 0.1004 - val_loss: 0.0057 - val_mae: 0.0697 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0120 - mae: 0.0880 - val_loss: 0.0049 - val_mae: 0.0639 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0148 - mae: 0.1047 - val_loss: 0.0033 - val_mae: 0.0525 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0156 - mae: 0.0996 - val_loss: 0.0075 - val_mae: 0.0818 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m7/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0130 - mae: 0.0911\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0129 - mae: 0.0902 - val_loss: 0.0084 - val_mae: 0.0871 - learning_rate: 5.0000e-04\n",
      "LSTM model trained successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0289\n",
      "MSE             │     0.0008\n",
      "MAE             │     0.0259\n",
      "R²              │    -0.1195\n",
      "MedAE           │     0.0212\n",
      "Max Error       │     0.0542\n",
      "MAD             │     0.0240\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm(restaurants_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error, median_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=20):  # Reduced sequence length\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to train optimized LSTM model\n",
    "def train_lstm(df):\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Normalize features using StandardScaler\n",
    "    feature_scaler = MaxAbsScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "    # Normalize the target variable\n",
    "    target_scaler = MaxAbsScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # Adjusted sequence length\n",
    "    sequence_length = 20  # Reduced from 25 to minimize noise\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Build Optimized LSTM Model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(64, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),  # Reduced neurons\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, return_sequences=False),  # Reduced second LSTM layer\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Predicting the scaled target variable\n",
    "    ])\n",
    "\n",
    "    # Compile model with optimized learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])  # Reduced LR to 0.0005\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    # Train the model with optimized parameters\n",
    "    history = model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32, validation_data=(X_test_seq, y_test_seq),\n",
    "                        verbose=1, callbacks=[early_stop, lr_scheduler])  # Reduced batch size to 32\n",
    "\n",
    "    print(\"\\n✅ LSTM Model Training Completed!\")\n",
    "\n",
    "    # Predictions (inverse transform back to original scale)\n",
    "    y_pred_scaled = model.predict(X_test_seq)\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled)  # Convert predictions back to original scale\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))  # Convert actual values back\n",
    "\n",
    "    # Compute Metrics\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "    mse = mean_squared_error(y_test_actual, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_actual, y_pred)\n",
    "    medae = median_absolute_error(y_test_actual, y_pred)\n",
    "    max_err = max_error(y_test_actual, y_pred)\n",
    "    mad = np.mean(np.abs(y_test_actual - np.mean(y_test_actual)))  # Mean Absolute Deviation\n",
    "\n",
    "    # Print Results in Table Format\n",
    "    print(f\"\\n{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 483ms/step - loss: 0.8243 - mae: 0.9062 - val_loss: 0.5850 - val_mae: 0.7645 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.5429 - mae: 0.7323 - val_loss: 0.2727 - val_mae: 0.5217 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2476 - mae: 0.4852 - val_loss: 0.0363 - val_mae: 0.1891 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0722 - mae: 0.2320 - val_loss: 0.0281 - val_mae: 0.1662 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0568 - mae: 0.2013 - val_loss: 0.0863 - val_mae: 0.2929 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0603 - mae: 0.2149 - val_loss: 0.0328 - val_mae: 0.1796 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0352 - mae: 0.1592 - val_loss: 0.0013 - val_mae: 0.0289 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0160 - mae: 0.0972 - val_loss: 0.0052 - val_mae: 0.0681 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0286 - mae: 0.1195 - val_loss: 0.0087 - val_mae: 0.0905 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0186 - mae: 0.1021 - val_loss: 0.0054 - val_mae: 0.0698 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0214 - mae: 0.1131 - val_loss: 9.4745e-04 - val_mae: 0.0268 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0116 - mae: 0.0863 - val_loss: 0.0011 - val_mae: 0.0252 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0126 - mae: 0.0929 - val_loss: 0.0019 - val_mae: 0.0376 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0137 - mae: 0.0934 - val_loss: 9.2895e-04 - val_mae: 0.0228 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0121 - mae: 0.0871 - val_loss: 5.6007e-04 - val_mae: 0.0202 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0131 - mae: 0.0931 - val_loss: 0.0010 - val_mae: 0.0282 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0137 - mae: 0.0882 - val_loss: 0.0015 - val_mae: 0.0343 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0132 - mae: 0.0896 - val_loss: 0.0015 - val_mae: 0.0335 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0160 - mae: 0.1068 - val_loss: 0.0012 - val_mae: 0.0306 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0117 - mae: 0.0829\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0115 - mae: 0.0824 - val_loss: 0.0012 - val_mae: 0.0301 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0115 - mae: 0.0809 - val_loss: 0.0012 - val_mae: 0.0303 - learning_rate: 2.5000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0139 - mae: 0.0920 - val_loss: 0.0012 - val_mae: 0.0304 - learning_rate: 2.5000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0121 - mae: 0.0850 - val_loss: 0.0013 - val_mae: 0.0312 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0088 - mae: 0.0754 - val_loss: 0.0014 - val_mae: 0.0324 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0123 - mae: 0.0838\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0120 - mae: 0.0830 - val_loss: 0.0017 - val_mae: 0.0360 - learning_rate: 2.5000e-04\n",
      "\n",
      "✅ LSTM Model Training Completed!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0237\n",
      "MSE             │     0.0006\n",
      "MAE             │     0.0202\n",
      "R²              │     0.1136\n",
      "MedAE           │     0.0201\n",
      "Max Error       │     0.0454\n",
      "MAD             │     0.0202\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm(restaurants_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error, median_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=20):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to train LSTM model\n",
    "def train_lstm(df):\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Normalize features using StandardScaler\n",
    "    feature_scaler = MaxAbsScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "    # Normalize the target variable\n",
    "    target_scaler = MaxAbsScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=SEED, shuffle=False)\n",
    "\n",
    "    # Sequence length\n",
    "    sequence_length = 20\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Build Optimized LSTM Model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(64, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile model with Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_seq, y_train_seq, epochs=100, batch_size=32, validation_data=(X_test_seq, y_test_seq),\n",
    "                        verbose=1, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "    print(\"\\n✅ LSTM Model Training Completed!\")\n",
    "\n",
    "    # Predictions (inverse transform back to original scale)\n",
    "    y_pred_scaled = model.predict(X_test_seq)\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "    # Compute Metrics\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "    mse = mean_squared_error(y_test_actual, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_actual, y_pred)\n",
    "    medae = median_absolute_error(y_test_actual, y_pred)\n",
    "    max_err = max_error(y_test_actual, y_pred)\n",
    "    mad = np.mean(np.abs(y_test_actual - np.mean(y_test_actual)))\n",
    "\n",
    "    # Print Results in Table Format\n",
    "    print(f\"\\n{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 488ms/step - loss: 0.8775 - mae: 0.9352 - val_loss: 0.6526 - val_mae: 0.8075 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6331 - mae: 0.7933 - val_loss: 0.3791 - val_mae: 0.6152 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3531 - mae: 0.5875 - val_loss: 0.1336 - val_mae: 0.3647 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1295 - mae: 0.3394 - val_loss: 0.0038 - val_mae: 0.0570 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0282 - mae: 0.1378 - val_loss: 0.0380 - val_mae: 0.1937 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0525 - mae: 0.1916 - val_loss: 0.0613 - val_mae: 0.2466 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0630 - mae: 0.2197 - val_loss: 0.0211 - val_mae: 0.1438 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0226 - mae: 0.1245 - val_loss: 0.0011 - val_mae: 0.0267 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0157 - mae: 0.0975 - val_loss: 0.0025 - val_mae: 0.0455 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0176 - mae: 0.1087 - val_loss: 0.0045 - val_mae: 0.0637 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0177 - mae: 0.1023 - val_loss: 0.0023 - val_mae: 0.0422 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0148 - mae: 0.0934 - val_loss: 6.3619e-04 - val_mae: 0.0219 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0112 - mae: 0.0880 - val_loss: 5.2113e-04 - val_mae: 0.0182 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0123 - mae: 0.0930 - val_loss: 7.2065e-04 - val_mae: 0.0235 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0097 - mae: 0.0830 - val_loss: 0.0023 - val_mae: 0.0428 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0111 - mae: 0.0843 - val_loss: 0.0058 - val_mae: 0.0727 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0102 - mae: 0.0764 - val_loss: 0.0074 - val_mae: 0.0829 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0054 - mae: 0.0584\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0055 - mae: 0.0593 - val_loss: 0.0056 - val_mae: 0.0711 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0088 - mae: 0.0745 - val_loss: 0.0047 - val_mae: 0.0638 - learning_rate: 2.5000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0086 - mae: 0.0693 - val_loss: 0.0043 - val_mae: 0.0610 - learning_rate: 2.5000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0078 - mae: 0.0695 - val_loss: 0.0042 - val_mae: 0.0600 - learning_rate: 2.5000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0080 - mae: 0.0726 - val_loss: 0.0050 - val_mae: 0.0662 - learning_rate: 2.5000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.0680\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0073 - mae: 0.0681 - val_loss: 0.0061 - val_mae: 0.0744 - learning_rate: 2.5000e-04\n",
      "\n",
      "✅ LSTM Model Training Completed!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0228\n",
      "MSE             │     0.0005\n",
      "MAE             │     0.0182\n",
      "R²              │     0.1752\n",
      "MedAE           │     0.0174\n",
      "Max Error       │     0.0459\n",
      "MAD             │     0.0202\n"
     ]
    }
   ],
   "source": [
    "model = train_lstm(restaurants_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - EXP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error, median_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from itertools import product\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=20):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to remove highly correlated features\n",
    "def remove_highly_correlated_features(df, threshold=0.9):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return df.drop(columns=to_drop), to_drop\n",
    "\n",
    "# Grid search function\n",
    "def grid_search_lstm(df):\n",
    "    X = df.drop(columns=['Popularity', 'Avg Rating - Business Type', 'Competition - Business Type/Area', \n",
    "                         'Competition - Food & Dining/Area', 'Competition - Business Type/POI Density', \n",
    "                         'Competition - Food & Dining/POI Density', 'Competition - Business Type/related POIs'])\n",
    "    y = df[\"Popularity\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "    X, removed_corr_features = remove_highly_correlated_features(X, threshold=0.9)\n",
    "    print(f\"Removed highly correlated features: {removed_corr_features}\")\n",
    "    \n",
    "    # Split dataset (80% train, 20% test) BEFORE applying transformations\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Apply PowerTransformer (normalize distribution) on training set only\n",
    "    num_cols = X.select_dtypes(include=['number']).columns\n",
    "    power_transformer = PowerTransformer()\n",
    "    \n",
    "    X_train[num_cols] = power_transformer.fit_transform(X_train[num_cols])\n",
    "    X_test[num_cols] = power_transformer.transform(X_test[num_cols])  # Apply same transformation\n",
    "\n",
    "    # Normalize features using MinMaxScaler (on numerical columns only)\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_train[num_cols] = feature_scaler.fit_transform(X_train[num_cols])\n",
    "    X_test[num_cols] = feature_scaler.transform(X_test[num_cols])\n",
    "\n",
    "    # Normalize the target variable separately\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Sequence length\n",
    "    sequence_length = 20\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train.values, y_train_scaled, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test.values, y_test_scaled, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Define hyperparameters for grid search\n",
    "    lstm_units = [32, 64, 128]\n",
    "    dropout_rates = [0.2, 0.3, 0.4]\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform grid search\n",
    "    for units, dropout, lr, batch_size in product(lstm_units, dropout_rates, learning_rates, batch_sizes):\n",
    "        print(f\"Training model with LSTM Units: {units}, Dropout: {dropout}, LR: {lr}, Batch Size: {batch_size}\")\n",
    "        \n",
    "        # Build LSTM model\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(units, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "            Dropout(dropout),\n",
    "            LSTM(units // 2, return_sequences=False),\n",
    "            Dropout(dropout),\n",
    "            Dense(units, activation='relu'),\n",
    "            Dense(units // 2, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=batch_size, validation_data=(X_test_seq, y_test_seq),\n",
    "                            verbose=0, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "        # Get best validation loss\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            best_params = {'LSTM Units': units, 'Dropout': dropout, 'Learning Rate': lr, 'Batch Size': batch_size}\n",
    "\n",
    "    print(\"\\n✅ Best Model Found!\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Make predictions with the best model\n",
    "    y_pred = best_model.predict(X_test_seq)\n",
    "\n",
    "    # Rescale predictions and actual values back to original scale\n",
    "    y_pred_original = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred_original)\n",
    "    mse = mean_squared_error(y_test_actual, y_pred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_actual, y_pred_original)\n",
    "\n",
    "    print(\"\\n📊 Best Model Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    return best_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed highly correlated features: ['Restaurants']\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "✅ Best Model Found!\n",
      "{'LSTM Units': 128, 'Dropout': 0.3, 'Learning Rate': 0.0001, 'Batch Size': 16}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 521ms/step\n",
      "\n",
      "📊 Best Model Metrics:\n",
      "RMSE: 0.0341, MSE: 0.0012, MAE: 0.0282, R²: 0.0265\n"
     ]
    }
   ],
   "source": [
    "model, best_params = grid_search_lstm(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed highly correlated features: ['Restaurants']\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "✅ Best Model Found!\n",
      "{'LSTM Units': 32, 'Dropout': 0.2, 'Learning Rate': 0.001, 'Batch Size': 32}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504ms/step\n",
      "\n",
      "📊 Best Model Metrics:\n",
      "RMSE: 0.0344, MSE: 0.0012, MAE: 0.0282, R²: 0.0056\n"
     ]
    }
   ],
   "source": [
    "model, best_params = grid_search_lstm(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed highly correlated features: ['Restaurants']\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "✅ Best Model Found!\n",
      "{'LSTM Units': 32, 'Dropout': 0.3, 'Learning Rate': 0.0005, 'Batch Size': 16}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step\n",
      "\n",
      "📊 Best Model Metrics:\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0343\n",
      "MSE             │     0.0012\n",
      "MAE             │     0.0287\n",
      "R²              │     0.0113\n",
      "MedAE           │     0.0284\n",
      "Max Error       │     0.0849\n",
      "MAD             │     0.0287\n"
     ]
    }
   ],
   "source": [
    "model, best_params = grid_search_lstm(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO POWERTRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error, median_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from itertools import product\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=20):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to remove highly correlated features\n",
    "def remove_highly_correlated_features(df, threshold=0.9):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return df.drop(columns=to_drop), to_drop\n",
    "\n",
    "# Grid search function\n",
    "def grid_search_lstm(df):\n",
    "    X = df.drop(columns=['Popularity', 'generalCategory', 'Avg Rating - Business Type', \n",
    "                         'Competition - Business Type/Area', 'Competition - Food & Dining/Area',\n",
    "                         'Competition - Business Type/POI Density', 'Competition - Food & Dining/POI Density',\n",
    "                         'Competition - Business Type/related POIs'])\n",
    "    y = df[\"Popularity\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "    X, removed_corr_features = remove_highly_correlated_features(X, threshold=0.9)\n",
    "    print(f\"Removed highly correlated features: {removed_corr_features}\")\n",
    "    \n",
    "    # Apply PowerTransformer (normalize distribution) only on training data\n",
    "    num_cols = X.select_dtypes(include=['number']).columns\n",
    "    power_transformer = PowerTransformer()\n",
    "    \n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Normalize features using MinMaxScaler (excluding specified columns)\n",
    "    ex_cols = ['Competition - Food & Dining/related POIs', 'Avg Rating - Food & Dining']\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    \n",
    "    cols_to_scale = [col for col in X_train.columns if col not in ex_cols]\n",
    "    X_train[cols_to_scale] = feature_scaler.fit_transform(X_train[cols_to_scale])\n",
    "    X_test[cols_to_scale] = feature_scaler.transform(X_test[cols_to_scale])  # Transform test set separately\n",
    "\n",
    "    # Sequence length\n",
    "    sequence_length = 20\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train.values, y_train.values, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test.values, y_test.values, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Define hyperparameters for grid search\n",
    "    lstm_units = [32, 64, 128]\n",
    "    dropout_rates = [0.2, 0.3, 0.4]\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform grid search\n",
    "    for units, dropout, lr, batch_size in product(lstm_units, dropout_rates, learning_rates, batch_sizes):\n",
    "        print(f\"Training model with LSTM Units: {units}, Dropout: {dropout}, LR: {lr}, Batch Size: {batch_size}\")\n",
    "        \n",
    "        # Build LSTM model\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(units, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "            Dropout(dropout),\n",
    "            LSTM(units // 2, return_sequences=False),\n",
    "            Dropout(dropout),\n",
    "            Dense(units, activation='relu'),\n",
    "            Dense(units // 2, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=batch_size, validation_data=(X_test_seq, y_test_seq),\n",
    "                            verbose=0, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "        # Get best validation loss\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            best_params = {'LSTM Units': units, 'Dropout': dropout, 'Learning Rate': lr, 'Batch Size': batch_size}\n",
    "\n",
    "    print(\"\\n✅ Best Model Found!\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Make predictions with the best model\n",
    "    y_pred = best_model.predict(X_test_seq).flatten()\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = mean_absolute_error(y_test_seq, y_pred)\n",
    "    mse = mean_squared_error(y_test_seq, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_seq, y_pred)\n",
    "    max_err = max_error(y_test_seq, y_pred)\n",
    "    medae = median_absolute_error(y_test_seq, y_pred)\n",
    "    mad = np.mean(np.abs(y_test_seq - y_pred))\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"\\n📊 Best Model Metrics:\")\n",
    "    print(f\"{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return best_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed highly correlated features: ['Restaurants']\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "✅ Best Model Found!\n",
      "{'LSTM Units': 32, 'Dropout': 0.3, 'Learning Rate': 0.001, 'Batch Size': 64}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 525ms/step\n",
      "\n",
      "📊 Best Model Metrics:\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0338\n",
      "MSE             │     0.0011\n",
      "MAE             │     0.0263\n",
      "R²              │     0.0429\n",
      "MedAE           │     0.0239\n",
      "Max Error       │     0.0984\n",
      "MAD             │     0.0263\n"
     ]
    }
   ],
   "source": [
    "model, best_params = grid_search_lstm(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - EXP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error, median_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from itertools import product\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=20):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Function to remove highly correlated features\n",
    "def remove_highly_correlated_features(df, threshold=0.9):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return df.drop(columns=to_drop), to_drop\n",
    "\n",
    "# Grid search function\n",
    "def grid_search_lstm(df):\n",
    "    X = df=[\"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"]\n",
    "    y = df[\"Popularity\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "    X, removed_corr_features = remove_highly_correlated_features(X, threshold=0.9)\n",
    "    print(f\"Removed highly correlated features: {removed_corr_features}\")\n",
    "    \n",
    "    # Apply PowerTransformer (normalize distribution)\n",
    "    num_cols = X.select_dtypes(include=['number']).columns\n",
    "    power_transformer = PowerTransformer()\n",
    "    X[num_cols] = power_transformer.fit_transform(X[num_cols])\n",
    "    \n",
    "    # Normalize features using MinMaxScaler\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X[X.columns] = feature_scaler.fit_transform(X)\n",
    "\n",
    "    # Normalize the target variable\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Sequence length\n",
    "    sequence_length = 20\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train.values, y_train, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test.values, y_test, sequence_length)\n",
    "\n",
    "    # Reshape input to match LSTM expectations\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Define hyperparameters for grid search\n",
    "    lstm_units = [32, 64, 128]\n",
    "    dropout_rates = [0.2, 0.3, 0.4]\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform grid search\n",
    "    for units, dropout, lr, batch_size in product(lstm_units, dropout_rates, learning_rates, batch_sizes):\n",
    "        print(f\"Training model with LSTM Units: {units}, Dropout: {dropout}, LR: {lr}, Batch Size: {batch_size}\")\n",
    "        \n",
    "        # Build LSTM model\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(units, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "            Dropout(dropout),\n",
    "            LSTM(units // 2, return_sequences=False),\n",
    "            Dropout(dropout),\n",
    "            Dense(units, activation='relu'),\n",
    "            Dense(units // 2, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=batch_size, validation_data=(X_test_seq, y_test_seq),\n",
    "                            verbose=0, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "        # Get best validation loss\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            best_params = {'LSTM Units': units, 'Dropout': dropout, 'Learning Rate': lr, 'Batch Size': batch_size}\n",
    "\n",
    "    print(\"\\n✅ Best Model Found!\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Make predictions with the best model\n",
    "    y_pred = best_model.predict(X_test_seq)\n",
    "\n",
    "    # Rescale predictions and actual values back to original scale\n",
    "    y_pred_original = target_scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred_original)\n",
    "    mse = mean_squared_error(y_test_actual, y_pred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_actual, y_pred_original)\n",
    "    max_err = max_error(y_test_actual, y_pred_original)\n",
    "    medae = median_absolute_error(y_test_actual, y_pred_original)\n",
    "    mad = np.mean(np.abs(y_test_actual - y_pred_original))\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"\\n📊 Best Model Metrics:\")\n",
    "    print(f\"{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, best_params = grid_search_lstm(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed highly correlated features: []\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 32, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 64, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.2, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.3, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 16\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 32\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.001, Batch Size: 64\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 16\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 32\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0005, Batch Size: 64\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 16\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 32\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Training model with LSTM Units: 128, Dropout: 0.4, LR: 0.0001, Batch Size: 64\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "✅ Best Model Found!\n",
      "{'LSTM Units': 64, 'Dropout': 0.4, 'Learning Rate': 0.0001, 'Batch Size': 64}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\n",
      "\n",
      "📊 Best Model Metrics:\n",
      "Metric          │      Value\n",
      "-----------------------------------\n",
      "RMSE            │     0.0265\n",
      "MSE             │     0.0007\n",
      "MAE             │     0.0209\n",
      "R²              │     0.0094\n",
      "MedAE           │     0.0199\n",
      "Max Error       │     0.0900\n",
      "MAD             │     0.0209\n"
     ]
    }
   ],
   "source": [
    "model, best_params = grid_search_lstm(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error, median_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from itertools import product\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to remove highly correlated features\n",
    "def remove_highly_correlated_features(X, threshold=0.9):\n",
    "    X_df = pd.DataFrame(X)\n",
    "    corr_matrix = X_df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return X_df.drop(columns=to_drop, errors='ignore')\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length=20):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Grid search function\n",
    "def grid_search_lstm(df):\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "\n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Remove highly correlated features\n",
    "    X = remove_highly_correlated_features(X)\n",
    "\n",
    "    # Normalize features using MinMaxScaler\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "    # Normalize the target variable\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Split dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define reduced hyperparameters for grid search\n",
    "    sequence_lengths = 20  \n",
    "    lstm_units = [32, 64, 128]\n",
    "    dropout_rates = [0.2, 0.3, 0.4]\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform grid search\n",
    "    for seq_length, units, dropout, lr, batch_size in product(sequence_lengths, lstm_units, dropout_rates, learning_rates, batch_sizes):\n",
    "        print(f\"Training model with Seq Length: {seq_length}, LSTM Units: {units}, Dropout: {dropout}, LR: {lr}, Batch Size: {batch_size}\")\n",
    "\n",
    "        # Create sequences for LSTM\n",
    "        X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_length)\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_length)\n",
    "\n",
    "        # Reshape input to match LSTM expectations\n",
    "        X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], seq_length, X_train.shape[1]))\n",
    "        X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], seq_length, X_train.shape[1]))\n",
    "\n",
    "        # Build LSTM model\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(units, return_sequences=True, input_shape=(seq_length, X_train.shape[1]))),\n",
    "            Dropout(dropout),\n",
    "            LSTM(units // 2, return_sequences=False),\n",
    "            Dropout(dropout),\n",
    "            Dense(units, activation='relu'),\n",
    "            Dense(units // 2, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=batch_size, validation_data=(X_test_seq, y_test_seq),\n",
    "                            verbose=0, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "        # Get best validation loss\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            best_params = {'Sequence Length': seq_length, 'LSTM Units': units, 'Dropout': dropout, 'Learning Rate': lr, 'Batch Size': batch_size}\n",
    "\n",
    "    print(\"\\n✅ Best Model Found!\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    y_pred_scaled = best_model.predict(X_test_seq)\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_test_actual = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "    mse = mean_squared_error(y_test_actual, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_actual, y_pred)\n",
    "    max_err = max_error(y_test_actual, y_pred)\n",
    "    medae = median_absolute_error(y_test_actual, y_pred)\n",
    "    mad = np.mean(np.abs(y_test_actual - y_pred))\n",
    "\n",
    "    print(\"\\nBest Model Metrics:\")\n",
    "    print(f\"{'Metric':<15} │ {'Value':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'RMSE':<15} │ {rmse:>10.4f}\")\n",
    "    print(f\"{'MSE':<15} │ {mse:>10.4f}\")\n",
    "    print(f\"{'MAE':<15} │ {mae:>10.4f}\")\n",
    "    print(f\"{'R²':<15} │ {r2:>10.4f}\")\n",
    "    print(f\"{'MedAE':<15} │ {medae:>10.4f}\")\n",
    "    print(f\"{'Max Error':<15} │ {max_err:>10.4f}\")\n",
    "    print(f\"{'MAD':<15} │ {mad:>10.4f}\")\n",
    "\n",
    "    return best_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m grid_search_lstm(combined_df)\n",
      "Cell \u001b[1;32mIn[14], line 82\u001b[0m, in \u001b[0;36mgrid_search_lstm\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     79\u001b[0m best_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_length, units, dropout, lr, batch_size \u001b[38;5;129;01min\u001b[39;00m product(sequence_lengths, lstm_units, dropout_rates, learning_rates, batch_sizes):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model with Seq Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LSTM Units: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dropout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Create sequences for LSTM\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = grid_search_lstm(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 111ms/step - loss: 0.3272 - learning_rate: 5.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0644 - learning_rate: 5.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0104 - learning_rate: 5.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0233 - learning_rate: 5.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0120 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0107 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0096 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0149 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0094 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0100 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0085 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0064 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0069 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0094 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0078 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0082 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0063 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0076 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0065 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0062 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0073 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0060 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0077 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0066 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0055 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0064 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0065 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0074 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0058 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0066 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0061 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0075 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0055 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0059 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0061 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0061 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0060 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0060 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0051 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0039 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0069 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0056 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0057 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0053 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0034 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0048 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0066 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0047 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0058 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 122\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Example usage (assuming df is your DataFrame)\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_lstm(restaurants_df)\n",
      "Cell \u001b[1;32mIn[12], line 89\u001b[0m, in \u001b[0;36mtrain_lstm\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     84\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_seq, y_train_seq, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     85\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_test_seq, y_test_seq), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     86\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stop, lr_scheduler])\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Predict on test set\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_seq)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Compute evaluation metrics\u001b[39;00m\n\u001b[0;32m     92\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test_seq, predictions)\n",
      "File \u001b[1;32mc:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\progbar.py:119\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    116\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     numdigits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mlog10(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    120\u001b[0m     bar \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(numdigits) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m (current, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m    121\u001b[0m     bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\x1b\u001b[39;00m\u001b[38;5;124m[1m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\x1b\u001b[39;00m\u001b[38;5;124m[0m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, target, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# LSTM Training Function\n",
    "def train_lstm(df):\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "    \n",
    "    # Remove outliers\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train-test split with shuffle=True\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "    shuffle = False\n",
    "    sequence_length = 40\n",
    "    learning_rate = 0.0005\n",
    "    batch_size = 16\n",
    "    dropout_rate = 0.1\n",
    "  # Lower learning rate to improve convergence\n",
    "\n",
    "    # Create sequences for training\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train.values, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test.values, sequence_length)\n",
    "\n",
    "    # Reshape data for LSTM\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Define LSTM model with reduced dropout\n",
    "    model = Sequential([\n",
    "    LSTM(32, return_sequences=True, input_shape=(sequence_length, X_train.shape[1])),\n",
    "    Dropout(0.1),\n",
    "    LSTM(16, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=tf.keras.losses.Huber())\n",
    "\n",
    "    # Callbacks for early stopping and learning rate reduction\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=batch_size,\n",
    "                        validation_data=(X_test_seq, y_test_seq), verbose=1,\n",
    "                        callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = model.predict(X_test_seq).flatten()\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mse = mean_squared_error(y_test_seq, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test_seq, predictions)\n",
    "    r2 = r2_score(y_test_seq, predictions)\n",
    "\n",
    "    print(f\"🔹 RMSE: {rmse:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # 📊 Plot training loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linestyle='dashed')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # 📊 Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(y_test_seq, label=\"Actual\", color='blue')\n",
    "    plt.plot(predictions, label=\"Predicted\", color='red', linestyle='dashed')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Popularity\")\n",
    "    plt.title(\"Actual vs. Predicted Popularity\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage (assuming df is your DataFrame)\n",
    "trained_model = train_lstm(restaurants_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def train_lstm(df):\n",
    "    results = []\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"].copy()\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    sequence_lengths = [20, 25, 30]\n",
    "    lstm_units_list = [128, 150]\n",
    "    batch_sizes = [16, 32]\n",
    "    learning_rates = [0.001, 0.0005]\n",
    "    \n",
    "    for sequence_length in sequence_lengths:\n",
    "        X_train_seq, y_train_seq = create_sequences(X_train, y_train.values, sequence_length)\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test, y_test.values, sequence_length)\n",
    "        \n",
    "        X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "        X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "        \n",
    "        for lstm_units in lstm_units_list:\n",
    "            for batch_size in batch_sizes:\n",
    "                for lr in learning_rates:\n",
    "                    model = Sequential([\n",
    "                        Bidirectional(LSTM(lstm_units, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "                        Dropout(0.3),\n",
    "                        Bidirectional(LSTM(lstm_units // 2, return_sequences=False)),\n",
    "                        Dropout(0.3),\n",
    "                        Dense(32, activation='relu'),\n",
    "                        Dense(16, activation='relu'),\n",
    "                        Dense(1)\n",
    "                    ])\n",
    "                    \n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr, clipvalue=1.0), \n",
    "                                  loss=tf.keras.losses.Huber())\n",
    "                    \n",
    "                    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "                    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "                    \n",
    "                    history = model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=batch_size, validation_data=(X_test_seq, y_test_seq),\n",
    "                                        verbose=0, callbacks=[early_stop, lr_scheduler])\n",
    "                    \n",
    "                    predictions = model.predict(X_test_seq)\n",
    "                    \n",
    "                    mse = mean_squared_error(y_test_seq, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_test_seq, predictions)\n",
    "                    r2 = r2_score(y_test_seq, predictions)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"Sequence Length\": sequence_length,\n",
    "                        \"LSTM Units\": lstm_units,\n",
    "                        \"Batch Size\": batch_size,\n",
    "                        \"Learning Rate\": lr,\n",
    "                        \"RMSE\": rmse,\n",
    "                        \"MSE\": mse,\n",
    "                        \"MAE\": mae,\n",
    "                        \"R²\": r2\n",
    "                    })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.sort_values(by=\"RMSE\").to_string(index=False))\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 545ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001799B6C6E80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017994DF14E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 554ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 554ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 559ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 529ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 554ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 575ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 542ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 546ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 538ms/step\n",
      " Sequence Length  LSTM Units  Batch Size  Learning Rate     RMSE      MSE      MAE        R²\n",
      "              20         150          32         0.0010 0.021680 0.000470 0.017342  0.255367\n",
      "              25         128          16         0.0010 0.022961 0.000527 0.020389  0.293622\n",
      "              20         128          32         0.0005 0.023894 0.000571 0.019470  0.095527\n",
      "              25         150          32         0.0010 0.023966 0.000574 0.022062  0.230476\n",
      "              25         150          16         0.0005 0.024136 0.000583 0.021936  0.219521\n",
      "              25         128          32         0.0010 0.024241 0.000588 0.020952  0.212690\n",
      "              20         150          16         0.0010 0.024719 0.000611 0.018768  0.031997\n",
      "              25         150          16         0.0010 0.024899 0.000620 0.022216  0.169392\n",
      "              25         150          32         0.0005 0.025509 0.000651 0.021801  0.128141\n",
      "              20         150          16         0.0005 0.025575 0.000654 0.021427 -0.036224\n",
      "              20         128          32         0.0010 0.026280 0.000691 0.022380 -0.094111\n",
      "              20         150          32         0.0005 0.026334 0.000693 0.019437 -0.098619\n",
      "              25         128          16         0.0005 0.026548 0.000705 0.021078  0.055673\n",
      "              20         128          16         0.0010 0.026680 0.000712 0.021436 -0.127667\n",
      "              20         128          16         0.0005 0.027175 0.000738 0.022750 -0.169966\n",
      "              25         128          32         0.0005 0.027973 0.000782 0.023549 -0.048402\n",
      "              30         128          32         0.0010 0.028808 0.000830 0.027243  0.169229\n",
      "              30         128          16         0.0010 0.029020 0.000842 0.027536  0.156935\n",
      "              30         150          16         0.0005 0.029344 0.000861 0.027455  0.138035\n",
      "              30         150          32         0.0010 0.029505 0.000871 0.026265  0.128534\n",
      "              30         150          16         0.0010 0.029587 0.000875 0.027033  0.123644\n",
      "              30         150          32         0.0005 0.030634 0.000938 0.027790  0.060560\n",
      "              30         128          32         0.0005 0.031626 0.001000 0.026601 -0.001299\n",
      "              30         128          16         0.0005 0.032099 0.001030 0.028190 -0.031455\n"
     ]
    }
   ],
   "source": [
    "lstm_model = train_lstm(restaurants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_IQR(X, y):\n",
    "    Q1 = X.quantile(0.25)\n",
    "    Q3 = X.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def train_optimized_lstm_2(df):\n",
    "    results = []\n",
    "\n",
    "    # Feature Selection\n",
    "    feature_cols = [\n",
    "        \"Religious Institutions\", \"Coffee Shops\", \"Food & Dining\", \"Restaurants\", \n",
    "        \"Home & Construction Services\", \"Entertainment & Recreation\", \"Retail & Shopping\",\n",
    "        \"Finance & Services\", \"Education\", \"Health\", \"Public & Government Services\",\n",
    "        \"Hotels & Hospitality\", \"Transportation & Travel\", \"Beauty & Wellness\", \"POI Density\",\n",
    "        \"Avg Rating - Food & Dining\", \"Competition - Food & Dining/related POIs\",\n",
    "        \"Population Within 1km\"\n",
    "    ]\n",
    "\n",
    "    # Ensure selected features exist in dataset\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[\"Popularity\"]\n",
    "\n",
    "    # Step 1: Remove Outliers (IQR Method)\n",
    "    X, y = remove_outliers_IQR(X, y)\n",
    "\n",
    "    # Step 2: Normalize Features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Step 3: Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # Step 4: Convert Data into Sequences\n",
    "    sequence_length = 30  # Increased from 20 to 30 for better long-term dependencies\n",
    "\n",
    "    def create_sequences(data, target, seq_length):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            X_seq.append(data[i:i + seq_length])  # Features sequence\n",
    "            y_seq.append(target[i + seq_length])  # Target at next timestep\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train.values, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test.values, sequence_length)\n",
    "\n",
    "    # Reshape data for LSTM input\n",
    "    X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "    X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], sequence_length, X_train.shape[1]))\n",
    "\n",
    "    # Step 5: Define Optimized LSTM Model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(256, return_sequences=True, input_shape=(sequence_length, X_train.shape[1]))),\n",
    "        Dropout(0.35),  # Slightly increased dropout for better regularization\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.35),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.1),  # Better than ReLU for small gradients\n",
    "        Dense(16),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dense(1)  # Output Layer\n",
    "    ])\n",
    "\n",
    "    # Compile Model with Optimized Hyperparameters\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005, clipvalue=1.0), \n",
    "              loss=tf.keras.losses.Huber())\n",
    "    \n",
    "    # Step 6: Implement Early Stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "    # Step 7: Train Model\n",
    "    history = model.fit(X_train_seq, y_train_seq, epochs=150, batch_size=16, validation_data=(X_test_seq, y_test_seq), \n",
    "                        verbose=1, callbacks=[early_stop])\n",
    "\n",
    "    # Step 8: Make Predictions\n",
    "    predictions = model.predict(X_test_seq)\n",
    "\n",
    "    # Step 9: Compute Metrics\n",
    "    mse = mean_squared_error(y_test_seq, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_seq, predictions)\n",
    "    r2 = r2_score(y_test_seq, predictions)\n",
    "\n",
    "    metrics = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2\n",
    "    }\n",
    "    results.append(metrics)\n",
    "\n",
    "    # Print Metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    print(pd.DataFrame(results).to_string(index=False))\n",
    "\n",
    "    # Step 10: Plot Training Loss\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('LSTM Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Step 11: Plot Predictions vs Actual\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(y_test_seq, label='Actual Popularity', color='royalblue')\n",
    "    plt.plot(predictions, label='Predicted Popularity', linestyle='dashed', color='orange')\n",
    "    plt.legend()\n",
    "    plt.title('LSTM Model: Actual vs Predicted Popularity')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Popularity')\n",
    "    plt.show()\n",
    "\n",
    "    return model  # Return trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "c:\\Users\\Jory\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 242ms/step - loss: 0.2408 - val_loss: 0.0549\n",
      "Epoch 2/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0283 - val_loss: 0.0029\n",
      "Epoch 3/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 616ms/step - loss: 0.0092 - val_loss: 0.0095\n",
      "Epoch 4/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - loss: 0.0094 - val_loss: 7.4353e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 0.0093 - val_loss: 0.0028\n",
      "Epoch 6/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 0.0062 - val_loss: 5.6088e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - loss: 0.0057 - val_loss: 0.0079\n",
      "Epoch 8/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - loss: 0.0086 - val_loss: 5.5410e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - loss: 0.0058 - val_loss: 0.0014\n",
      "Epoch 10/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - loss: 0.0046 - val_loss: 0.0022\n",
      "Epoch 11/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0054 - val_loss: 0.0010\n",
      "Epoch 12/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0040 - val_loss: 0.0015\n",
      "Epoch 13/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0040 - val_loss: 0.0015\n",
      "Epoch 14/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0044 - val_loss: 8.4977e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0061 - val_loss: 0.0047\n",
      "Epoch 16/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - loss: 0.0040 - val_loss: 6.2460e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - loss: 0.0049 - val_loss: 0.0047\n",
      "Epoch 18/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.0046 - val_loss: 4.8629e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0036 - val_loss: 0.0010\n",
      "Epoch 20/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0032 - val_loss: 0.0020\n",
      "Epoch 21/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 22/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0037 - val_loss: 5.7466e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0044 - val_loss: 7.8466e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 25/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0035 - val_loss: 4.7840e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0037 - val_loss: 8.5177e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0032 - val_loss: 0.0010\n",
      "Epoch 28/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0034 - val_loss: 0.0059\n",
      "Epoch 29/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0038 - val_loss: 8.7306e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0040 - val_loss: 0.0014\n",
      "Epoch 31/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 32/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0040 - val_loss: 7.0871e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 34/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 35/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 0.0046 - val_loss: 7.5031e-04\n",
      "Epoch 36/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 37/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0038 - val_loss: 6.2974e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0041 - val_loss: 0.0015\n",
      "Epoch 39/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0037 - val_loss: 4.8862e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0030 - val_loss: 9.5451e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0031 - val_loss: 8.9147e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0043 - val_loss: 0.0034\n",
      "Epoch 43/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0052 - val_loss: 0.0013\n",
      "Epoch 44/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0034 - val_loss: 0.0026\n",
      "Epoch 45/150\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0033 - val_loss: 9.1979e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "Final Metrics:\n",
      "    RMSE      MSE      MAE       R²\n",
      "0.030932 0.000957 0.027992 0.042179\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_model = train_optimized_lstm_2(restaurants_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
